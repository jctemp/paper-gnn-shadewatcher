

@misc{shadewatcherrepo2022,
  author = {Zengy, Jun and Wang, Xiang and Liu, Jiahao and Chen, Yinfang and Liang, Zhenkai and Chua, Tat-Seng and Chua, Zheng Leong},
  title = {ShadeWatcher},
  year = {2013},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/jun-zeng/ShadeWatcher},
  commit = {0031dfbb354b7cdfdbe9cbc52371f78c9ab83bd4}
}

@article{knowledge_graph_reasoning2020,
title = {A review: Knowledge reasoning over knowledge graph},
journal = {Expert Systems with Applications},
volume = {141},
pages = {112948},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112948},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419306669},
author = {Xiaojun Chen and Shengbin Jia and Yang Xiang},
keywords = {Knowledge graph, Reasoning, Rule-based reasoning, Distributed representation-based reasoning, Neural network-based reasoning},
abstract = {Mining valuable hidden knowledge from large-scale data relies on the support of reasoning technology. Knowledge graphs, as a new type of knowledge representation, have gained much attention in natural language processing. Knowledge graphs can effectively organize and represent knowledge so that it can be efficiently utilized in advanced applications. Recently, reasoning over knowledge graphs has become a hot research topic, since it can obtain new knowledge and conclusions from existing data. Herein we review the basic concept and definitions of knowledge reasoning and the methods for reasoning over knowledge graphs. Specifically, we dissect the reasoning methods into three categories: rule-based reasoning, distributed representation-based reasoning and neural network-based reasoning. We also review the related applications of knowledge graph reasoning, such as knowledge graph completion, question answering, and recommender systems. Finally, we discuss the remaining challenges and research opportunities for knowledge graph reasoning.}
}

@article{bipartite_graph2007,
title = {Bipartite network projection and personal recommendation},
author = {Zhou, Tao and Ren, Jie and Medo, Mat\'u\ifmmode \check{s}\else \v{s}\fi{} and Zhang, Yi-Cheng},
journal = {Phys. Rev. E},
volume = {76},
issue = {4},
pages = {046115},
numpages = {7},
year = {2007},
month = {Oct},
publisher = {American Physical Society},
doi = {10.1103/PhysRevE.76.046115},
url = {https://link.aps.org/doi/10.1103/PhysRevE.76.046115}
}

@inproceedings{transe2013,
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
booktitle = {Advances in Neural Information Processing Systems},
editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
pages = {},
publisher = {Curran Associates, Inc.},
title = {Translating Embeddings for Modeling Multi-relational Data},
url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
volume = {26},
year = {2013}
}

@article{transh2014, 
title={Knowledge Graph Embedding by Translating on Hyperplanes},
volume={28},
url={https://ojs.aaai.org/index.php/AAAI/article/view/8870},
DOI={10.1609/aaai.v28i1.8870},
abstractNote={ &lt;p&gt; We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling.We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up. &lt;/p&gt; }
number={1},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
year={2014},
month={Jun.}
}

@article{transr2015,
title={Learning Entity and Relation Embeddings for Knowledge Graph Completion},
volume={29},
url={https://ojs.aaai.org/index.php/AAAI/article/view/9491},
DOI={10.1609/aaai.v29i1.9491},
abstractNote={ &lt;p&gt; Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. &lt;/p&gt; },
number={1},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
year={2015},
month={Feb.}
}